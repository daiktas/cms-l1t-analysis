#!/usr/bin/env python
import click
import click_log
import glob
import logging
import pandas as pd
import re
import os
import sys
from tabulate import tabulate
import shutil

from cmsl1t.batch import Batch, Status, condor_status, lsf_status

if sys.version_info[0] < 3:
    from StringIO import StringIO
else:
    from io import StringIO

logger = logging.getLogger(__name__)
click_log.basic_config(logger)


def update_job_status(row):
    batch = row.batch
    batch_id = row.batch_id
    job_path = row.output_folder
    job_status = row.status
    if job_status == Status.FINISHED:
        return job_status
    else:
        # check if output files exist
        if has_output_files(job_path):
            return 'FINISHED'

        batch_status = get_batch_status(batch, batch_id)
        if batch_status:
            return batch_status
        else:
            return Status.FAILED


def has_output_files(job_path):
    path = os.path.join(job_path, 'plots-v1', '*.root')
    output_files = glob.glob(path)
    return output_files


def get_batch_status(batch, batch_id):
    if batch == Batch.lsf:
        return lsf_status(batch_id)
    elif batch == Batch.condor:
        return condor_status(batch_id)
    logging.error('Unknown batch system "{0}"'.format(batch))
    return Status.UNKNOWN


def job_summary(series, max_width=80):
    def wrap_format(x, y):
        string = '{0}, {1}'.format(x, y)
        # TODO: make sure to insert a line break (\n) for every max_width
        # characters (after the next comma)
        # all_commas = string.find_all(',')
        # insert every max_width if not already present
        return string
    return reduce(wrap_format, series)


@click.command()
@click.option(
    '-i', '--info_file', help='path to info.csv', type=click.Path(exists=True),
    required=True,
)
@click.option('-s', '--summary', help='print summary', default=False, is_flag=True)
@click_log.simple_verbosity_option(logger)
def run(info_file, summary):
    df = pd.read_csv(info_file, dtype={"batch_id": str})

    df['status'] = df.apply(update_job_status, axis=1)
    # make backup file
    shutil.copy(info_file, info_file + '.bak')
    # update status file
    df.to_csv(info_file, index=False)

    if summary:
        summarised_df = df.groupby(['status']).agg(dict(
            batch_id=['count', job_summary],
        ))
        print(tabulate(summarised_df, headers=['status', '# jobs', 'job IDs']))
    else:
        display_columns = ['local_id', 'batch_id', 'status', 'output_folder']
        df['output_folder'] = df['batch_dir'] + df['output_folder']
        print(tabulate(df[display_columns],
                       headers='keys', tablefmt='psql', showindex=False))


if __name__ == '__main__':
    run()
